from __future__ import annotations
"""TW3 Chat Backend â€“ FastAPI + Qwen 7B"""

import logging
import asyncio
from typing import Dict
from contextlib import asynccontextmanager
from functools import lru_cache
from pathlib import Path
from uuid import uuid4
from datetime import datetime, timedelta, timezone

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import pipeline, logging as hf_logging  # type: ignore

from search_tools import search_news_async

# â”€â”€â”€â”€â”€ Logger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("tw3.chat")

# RÃ©duit la verbositÃ© des warnings Transformers
hf_logging.set_verbosity_error()

# â”€â”€â”€â”€â”€ Pydantic DTO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AskIn(BaseModel):
    question: str = Field(..., min_length=3)
    conv_id: str | None = None  # ID de conversation, None pour le 1er tour

class AskOut(BaseModel):
    conv_id: str                      # renvoyÃ© au front
    answer: str

# --------------------------------------------------------

# dossier oÃ¹ lâ€™on Ã©crit les journaux (crÃ©Ã© au boot)
LOG_DIR = Path("/app/volume/conversations")
LOG_DIR.mkdir(parents=True, exist_ok=True)

def _append_log(conv_id: str, header_dt: str, role: str, text: str) -> None:
    """
    Ajoute une ligne au fichier de conversation.
    CrÃ©e le fichier + un en-tÃªte datÃ© si câ€™est le premier appel.
    """
    file = LOG_DIR / f"conv-{conv_id}_{header_dt}.txt"
    is_new = not file.exists()
    now = datetime.now(timezone.utc)
    ts = now.isoformat(timespec="seconds").replace("+00:00", "Z")
    with file.open("a", encoding="utf-8") as f:
        if is_new:
            f.write(f"# Conversation {conv_id} â€“ dÃ©marrÃ©e le {header_dt}\n\n")
        f.write(f"[{ts}] {role.upper()}: {text}\n")


# â”€â”€â”€â”€â”€ Hugging Face pipeline (lazyâ€‘load + cache) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@lru_cache(maxsize=1)
def get_pipe():
    """Charge le modÃ¨le Qwen 7B une seule fois (GPU si dispo).
    Utilise un cache LRU pour Ã©viter de recharger le modÃ¨le Ã  chaque appel.
    Returns:
        pipeline: Instance de pipeline pour la gÃ©nÃ©ration de texte.
    """
    logger.info("Loading Qwen pipelineâ€¦ (first call only)")
    return pipeline(
        "text-generation",
        model="Qwen/Qwen2.5-Coder-7B-Instruct",
        device_map="auto",       # GPU si prÃ©sent, sinon CPU
        trust_remote_code=True    # nÃ©cessaire pour Qwen
    )

def generate_answer(prompt: str,
                    max_new_tokens: int = 4096,
                    temperature: float = 0.7) -> str:
    """Appelle le modÃ¨le et rÃ©cupÃ¨re la rÃ©ponse texte.
    Args:
        prompt (str): La question ou le prompt Ã  envoyer au modÃ¨le.
        max_new_tokens (int): Nombre maximum de tokens Ã  gÃ©nÃ©rer.
        temperature (float): ContrÃ´le la crÃ©ativitÃ© de la gÃ©nÃ©ration.
    Returns:
        str: La rÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le.
    Raises:
        Exception: Si le modÃ¨le ne peut pas Ãªtre appelÃ© ou si la rÃ©ponse est invalide.
    """
    pipe = get_pipe()
    messages = [{"role": "user", "content": prompt}]
    out = pipe(
        messages,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
    )
    data = out[0]["generated_text"]

    # Format 1 : Qwen renvoie directement une str
    if isinstance(data, str):
        return data.strip()

    # Format 2 : liste de messages
    if isinstance(data, list):
        for msg in reversed(data):
            if isinstance(msg, dict) and msg.get("role") == "assistant":
                return str(msg.get("content", "")).strip()
        return " ".join(
            str(m.get("content", "")) if isinstance(m, dict) else str(m)
            for m in data
        ).strip()

    logger.warning("Unexpected generated_text type: %s", type(data))
    return str(data).strip()

# â”€â”€â”€â”€â”€ Lifespan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@asynccontextmanager
async def lifespan(app: FastAPI):
    """PrÃ©â€‘charge le pipeline au dÃ©marrage pour Ã©viter la latence du 1áµ‰Ê³ appel.
    Utilise un cache LRU pour garder une seule instance en mÃ©moire.
    Yields:
        None: Le contexte de l'application FastAPI.
    """
    logger.info("Preloading Qwen pipeline for faster responsesâ€¦")
    get_pipe()  # dÃ©clenche le cache LRU
    yield

# â”€â”€â”€â”€â”€ FastAPI app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(title="TW3 Chat Backend", lifespan=lifespan)

# â”€â”€â”€â”€â”€ Middleware â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# â”€â”€â”€â”€â”€ Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/", tags=["Health"])
def root() -> Dict[str, str]:
    """Simple endpoint de santÃ©.
    Returns:
        Dict[str, str]: Message de bienvenue.
    """
    logger.info("Health check endpoint called")
    return {"data": "Bienvenue sur l'API TW3 Chat ðŸŽ‰"}

@app.post("/ask", response_model=AskOut, tags=["Chat"])
async def ask_handler(payload: AskIn) -> AskOut:
    conv_id = payload.conv_id or str(uuid4())
    now = datetime.now(timezone.utc)
    header_dt = now.strftime("%Y-%m-%dT%H%M%S")
    question = payload.question.strip()
    logger.info("Conv %s â€“ question : %sâ€¦", conv_id, question)
    _append_log(conv_id, header_dt, "user", question)

    # 1) Recherche dâ€™actualitÃ©s asynchrone (search_news_async)
    from_date = (now - timedelta(days=7)).strftime("%Y-%m-%d")
    news_ctx = await search_news_async(question, from_date, "relevancy", max_results=5)
    logger.info("Conv %s â€“ found %d news articles", conv_id, news_ctx.count("- "))
    _append_log(conv_id, header_dt, "news", news_ctx or "Aucune information dâ€™actualitÃ© trouvÃ©e.")

    # 2) Prompt directif pour forcer lâ€™usage du contexte news
    if news_ctx:
        prompt = (
            f"RÃ©ponds Ã  la question suivante uniquement Ã  partir des articles dâ€™actualitÃ© ci-dessous."
            f"Nâ€™invente rien, ne donne pas dâ€™explications gÃ©nÃ©rales."
            f"Ne commence jamais par une excuse de type â€˜en tant quâ€™IA, je nâ€™ai pas accÃ¨s au webâ€™."
            f"Si aucune information des articles ne permet de rÃ©pondre, Ã©crisÂ : Â«Â Aucune information dâ€™actualitÃ© pertinente trouvÃ©e dans les articles fournis.Â Â»\n\n"
            f"Question : {question}\n\n"
            f"---\n"
            f"Articles dâ€™actualitÃ© Ã  exploiter :\n"
            f"{news_ctx}\n"
            f"---\n"
            "RÃ©ponse :"
        )
    else:
        prompt = question

    # 3) GÃ©nÃ©ration Qwen dans un thread sÃ©parÃ©
    loop = asyncio.get_running_loop()
    answer = await loop.run_in_executor(
        None,
        lambda: generate_answer(prompt)
    )

    _append_log(conv_id, header_dt, "bot", answer)
    return AskOut(conv_id=conv_id, answer=answer)