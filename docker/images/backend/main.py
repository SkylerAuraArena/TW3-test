from __future__ import annotations
"""TW3 Chat Backend ‚Äì FastAPI + Qwen 7B"""

import logging
import asyncio
from typing import Dict
from contextlib import asynccontextmanager
from functools import lru_cache

from fastapi import FastAPI
from pydantic import BaseModel, Field
from transformers import pipeline, logging as hf_logging # type: ignore

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Logger ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("tw3.chat")

# R√©duit la verbosit√© des warnings Transformers
hf_logging.set_verbosity_error()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Pydantic DTO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AskIn(BaseModel):
    question: str = Field(..., min_length=3)

class AskOut(BaseModel):
    answer: str

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hugging Face pipeline (lazy‚Äëload + cache) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@lru_cache(maxsize=1)
def get_pipe():
    """Charge le mod√®le Qwen 7B une seule fois (GPU si dispo).
    Utilise un cache LRU pour √©viter de recharger le mod√®le √† chaque appel.
    Returns:
        pipeline: Instance de pipeline pour la g√©n√©ration de texte.
    """
    logger.info("Loading Qwen pipeline‚Ä¶ (first call only)")
    return pipeline(
        "text-generation",
        model="Qwen/Qwen2.5-Coder-7B-Instruct",
        device_map="auto",       # GPU si pr√©sent, sinon CPU
        trust_remote_code=True    # n√©cessaire pour Qwen
    )

def generate_answer(question: str,
                    max_new_tokens: int = 128,
                    temperature: float = 0.2) -> str:
    """G√©n√®re une r√©ponse √† *question* via le mod√®le Qwen.
    G√®re les deux formats de sortie possibles :
    1. `generated_text` est une **str**  ‚Äì cas habituel.
    2. `generated_text` est une **liste de messages** (dict) ‚Äì certains templates r√©cents renvoient toute la conversation.
    Args:
        question (str): La question √† laquelle r√©pondre.
        max_new_tokens (int): Nombre maximum de tokens √† g√©n√©rer.
        temperature (float): Contr√¥le la cr√©ativit√© de la r√©ponse.
    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le.
    """
    logger.info(f"Generating answer for question: {question}")
    pipe = get_pipe()
    messages = [{"role": "user", "content": question}]
    out = pipe(
        messages,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
    )
    data = out[0]["generated_text"]

    # Format 1 : cha√Æne de caract√®res directe
    if isinstance(data, str):
        return data.strip()

    # Format 2 : liste de messages (dict)
    if isinstance(data, list):
        # On cherche le dernier message de r√¥le 'assistant'
        for msg in reversed(data):
            if isinstance(msg, dict) and msg.get("role") == "assistant":
                return str(msg.get("content", "")).strip()
        # Fallback : tout convertir en texte
        return " ".join(str(m.get("content", "")) if isinstance(m, dict) else str(m) for m in data).strip()

    # Inattendu : on cast en str
    logger.warning("Unexpected generated_text type: %s", type(data))
    return str(data).strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Lifespan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Pr√©‚Äëcharge le pipeline au d√©marrage pour √©viter la latence du 1·µâ ≥ appel.
    Utilise un cache LRU pour garder une seule instance en m√©moire.
    Yields:
        None: Le contexte de l'application FastAPI.
    """
    logger.info("Preloading Qwen pipeline for faster responses‚Ä¶")
    get_pipe()  # d√©clenche le cache LRU
    yield

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI app ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app = FastAPI(title="TW3 Chat Backend", lifespan=lifespan)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@app.get("/", tags=["Health"])
def root() -> Dict[str, str]:
    """Simple endpoint de sant√©.
    Returns:
        Dict[str, str]: Message de bienvenue.
    """
    logger.info("Health check endpoint called")
    return {"data": "Bienvenue sur l'API TW3 Chat üéâ"}


@app.post("/ask", response_model=AskOut, tags=["Chat"])
async def ask_handler(payload: AskIn) -> AskOut:
    """Retourne la r√©ponse g√©n√©r√©e localement par Qwen.
    Args:
        payload (AskIn): Contient la question √† poser.
    Returns:
        AskOut: Contient la r√©ponse g√©n√©r√©e par le mod√®le.
    """
    logger.info(f"Received question: {payload.question}")
    question = payload.question.strip()

    # Ex√©cution bloquante d√©port√©e dans un thread pour ne pas bloquer l'event‚Äëloop
    loop = asyncio.get_running_loop()
    answer = await loop.run_in_executor(
        None,
        lambda: generate_answer(question, max_new_tokens=256, temperature=0.7)
    )

    return AskOut(answer=answer)