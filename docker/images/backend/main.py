from __future__ import annotations
"""TW3 Chat Backend ‚Äì FastAPI + Qwen 7B"""

import logging
import asyncio
from typing import Dict
from contextlib import asynccontextmanager
from functools import lru_cache

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import pipeline, logging as hf_logging # type: ignore

from pathlib import Path
from uuid import uuid4
from datetime import datetime

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Logger ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("tw3.chat")

# R√©duit la verbosit√© des warnings Transformers
hf_logging.set_verbosity_error()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Pydantic DTO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AskIn(BaseModel):
    question: str = Field(..., min_length=3)
    conv_id: str | None = None  # ID de conversation, None pour le 1er tour
class AskOut(BaseModel):
    conv_id: str                      # renvoy√© au front
    answer: str

# --------------------------------------------------------

# dossier o√π l‚Äôon √©crit les journaux (cr√©√© au boot)
LOG_DIR = Path("/app/volume/conversations")
LOG_DIR.mkdir(parents=True, exist_ok=True)

def _append_log(conv_id: str, header_dt: str, role: str, text: str) -> None:
    """
    Ajoute une ligne au fichier de conversation.  
    Cr√©e le fichier + un en-t√™te dat√© si c‚Äôest le premier appel.
    """
    file = LOG_DIR / f"conv-{conv_id}_{header_dt}.txt"

    is_new = not file.exists()
    with file.open("a", encoding="utf-8") as f:
        if is_new:
            f.write(f"# Conversation {conv_id} ‚Äì d√©marr√©e le {header_dt}\n\n")
        ts = datetime.utcnow().isoformat(timespec="seconds") + "Z"
        f.write(f"[{ts}] {role.upper()}: {text}\n")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hugging Face pipeline (lazy‚Äëload + cache) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@lru_cache(maxsize=1)
def get_pipe():
    """Charge le mod√®le Qwen 7B une seule fois (GPU si dispo).
    Utilise un cache LRU pour √©viter de recharger le mod√®le √† chaque appel.
    Returns:
        pipeline: Instance de pipeline pour la g√©n√©ration de texte.
    """
    logger.info("Loading Qwen pipeline‚Ä¶ (first call only)")
    return pipeline(
        "text-generation",
        model="Qwen/Qwen2.5-Coder-7B-Instruct",
        device_map="auto",       # GPU si pr√©sent, sinon CPU
        trust_remote_code=True    # n√©cessaire pour Qwen
    )

def generate_answer(question: str,
                    max_new_tokens: int = 128,
                    temperature: float = 0.2) -> str:
    """G√©n√®re une r√©ponse √† *question* via le mod√®le Qwen.
    G√®re les deux formats de sortie possibles :
    1. `generated_text` est une **str**  ‚Äì cas habituel.
    2. `generated_text` est une **liste de messages** (dict) ‚Äì certains templates r√©cents renvoient toute la conversation.
    Args:
        question (str): La question √† laquelle r√©pondre.
        max_new_tokens (int): Nombre maximum de tokens √† g√©n√©rer.
        temperature (float): Contr√¥le la cr√©ativit√© de la r√©ponse.
    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le.
    """
    logger.info(f"Generating answer for question: {question}")
    pipe = get_pipe()
    messages = [{"role": "user", "content": question}]
    out = pipe(
        messages,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
    )
    data = out[0]["generated_text"]

    # Format 1 : cha√Æne de caract√®res directe
    if isinstance(data, str):
        return data.strip()

    # Format 2 : liste de messages (dict)
    if isinstance(data, list):
        # On cherche le dernier message de r√¥le 'assistant'
        for msg in reversed(data):
            if isinstance(msg, dict) and msg.get("role") == "assistant":
                return str(msg.get("content", "")).strip()
        # Fallback : tout convertir en texte
        return " ".join(str(m.get("content", "")) if isinstance(m, dict) else str(m) for m in data).strip()

    # Inattendu : on cast en str
    logger.warning("Unexpected generated_text type: %s", type(data))
    return str(data).strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Lifespan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Pr√©‚Äëcharge le pipeline au d√©marrage pour √©viter la latence du 1·µâ ≥ appel.
    Utilise un cache LRU pour garder une seule instance en m√©moire.
    Yields:
        None: Le contexte de l'application FastAPI.
    """
    logger.info("Preloading Qwen pipeline for faster responses‚Ä¶")
    get_pipe()  # d√©clenche le cache LRU
    yield

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI app ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app = FastAPI(title="TW3 Chat Backend", lifespan=lifespan)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Middleware ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],              # ‚Ü≥ restreins √† ["http://localhost:3000"] si tu pr√©f√®res
    allow_credentials=False,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@app.get("/", tags=["Health"])
def root() -> Dict[str, str]:
    """Simple endpoint de sant√©.
    Returns:
        Dict[str, str]: Message de bienvenue.
    """
    logger.info("Health check endpoint called")
    return {"data": "Bienvenue sur l'API TW3 Chat üéâ"}


@app.post("/ask", response_model=AskOut, tags=["Chat"])
async def ask_handler(payload: AskIn) -> AskOut:
    """Endpoint pour poser une question et obtenir une r√©ponse.
    Args:
        payload (AskIn): Contient la question et l'ID de conversation.
    Returns:
        AskOut: Contient l'ID de conversation et la r√©ponse g√©n√©r√©e.
    """
    conv_id = payload.conv_id or str(uuid4())
    header_dt = datetime.utcnow().strftime("%Y-%m-%dT%H%M%S")

    question = payload.question.strip()
    logger.info("Generating answer for conv %s ‚Äì q='%s‚Ä¶'", conv_id, question[:60])

    # 1) log la QUESTION
    _append_log(conv_id, header_dt, "user", question)

    # 2) g√©n√©ration (inchang√©)
    loop = asyncio.get_running_loop()
    answer = await loop.run_in_executor(
        None,
        lambda: generate_answer(question, max_new_tokens=256, temperature=0.7),
    )

    # 3) log la R√âPONSE
    _append_log(conv_id, header_dt, "bot", answer)

    return AskOut(conv_id=conv_id, answer=answer)