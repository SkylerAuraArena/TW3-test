from __future__ import annotations
"""TW3 Chat Backend ‚Äì FastAPI + Qwen 7B"""

import logging
import asyncio
from typing import Dict
from contextlib import asynccontextmanager
from functools import lru_cache
from pathlib import Path
from uuid import uuid4
from datetime import datetime

import httpx      
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import pipeline, logging as hf_logging # type: ignore

from search_tools import search_web

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Logger ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("tw3.chat")

# R√©duit la verbosit√© des warnings Transformers
hf_logging.set_verbosity_error()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Pydantic DTO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AskIn(BaseModel):
    question: str = Field(..., min_length=3)
    conv_id: str | None = None  # ID de conversation, None pour le 1er tour
class AskOut(BaseModel):
    conv_id: str                      # renvoy√© au front
    answer: str

# --------------------------------------------------------

# dossier o√π l‚Äôon √©crit les journaux (cr√©√© au boot)
LOG_DIR = Path("/app/volume/conversations")
LOG_DIR.mkdir(parents=True, exist_ok=True)

def _append_log(conv_id: str, header_dt: str, role: str, text: str) -> None:
    """
    Ajoute une ligne au fichier de conversation.  
    Cr√©e le fichier + un en-t√™te dat√© si c‚Äôest le premier appel.
    Args:
        conv_id (str): ID de la conversation.
        header_dt (str): Date/heure pour l'en-t√™te du fichier.
        role (str): R√¥le de l'auteur du message ('user' ou 'bot').
        text (str): Le texte du message √† enregistrer.
    Returns:
        None
    Raises:
        OSError: Si le fichier ne peut pas √™tre √©crit.
    """
    file = LOG_DIR / f"conv-{conv_id}_{header_dt}.txt"

    is_new = not file.exists()
    with file.open("a", encoding="utf-8") as f:
        if is_new:
            f.write(f"# Conversation {conv_id} ‚Äì d√©marr√©e le {header_dt}\n\n")
        ts = datetime.utcnow().isoformat(timespec="seconds") + "Z"
        f.write(f"[{ts}] {role.upper()}: {text}\n")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hugging Face pipeline (lazy‚Äëload + cache) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@lru_cache(maxsize=1)
def get_pipe():
    """Charge le mod√®le Qwen 7B une seule fois (GPU si dispo).
    Utilise un cache LRU pour √©viter de recharger le mod√®le √† chaque appel.
    Returns:
        pipeline: Instance de pipeline pour la g√©n√©ration de texte.
    """
    logger.info("Loading Qwen pipeline‚Ä¶ (first call only)")
    return pipeline(
        "text-generation",
        model="Qwen/Qwen2.5-Coder-7B-Instruct",
        device_map="auto",       # GPU si pr√©sent, sinon CPU
        trust_remote_code=True    # n√©cessaire pour Qwen
    )

def generate_answer(prompt: str,
                    max_new_tokens: int = 256,
                    temperature: float = 0.7) -> str:
    """Appelle le mod√®le et r√©cup√®re la r√©ponse texte.
    Args:
        prompt (str): La question ou le prompt √† envoyer au mod√®le.
        max_new_tokens (int): Nombre maximum de tokens √† g√©n√©rer.
        temperature (float): Contr√¥le la cr√©ativit√© de la g√©n√©ration.
    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le.
    Raises:
        Exception: Si le mod√®le ne peut pas √™tre appel√© ou si la r√©ponse est invalide.
    """
    pipe = get_pipe()
    messages = [{"role": "user", "content": prompt}]
    out = pipe(
        messages,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
    )
    data = out[0]["generated_text"]

    # Format 1 : Qwen renvoie directement une str
    if isinstance(data, str):
        return data.strip()

    # Format 2 : liste de messages
    if isinstance(data, list):
        for msg in reversed(data):
            if isinstance(msg, dict) and msg.get("role") == "assistant":
                return str(msg.get("content", "")).strip()
        return " ".join(
            str(m.get("content", "")) if isinstance(m, dict) else str(m)
            for m in data
        ).strip()

    logger.warning("Unexpected generated_text type: %s", type(data))
    return str(data).strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Lifespan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Pr√©‚Äëcharge le pipeline au d√©marrage pour √©viter la latence du 1·µâ ≥ appel.
    Utilise un cache LRU pour garder une seule instance en m√©moire.
    Yields:
        None: Le contexte de l'application FastAPI.
    """
    logger.info("Preloading Qwen pipeline for faster responses‚Ä¶")
    get_pipe()  # d√©clenche le cache LRU
    yield

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI app ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app = FastAPI(title="TW3 Chat Backend", lifespan=lifespan)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Middleware ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],              # ‚Ü≥ restreins √† ["http://localhost:3000"] si tu pr√©f√®res
    allow_credentials=False,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@app.get("/", tags=["Health"])
def root() -> Dict[str, str]:
    """Simple endpoint de sant√©.
    Returns:
        Dict[str, str]: Message de bienvenue.
    """
    logger.info("Health check endpoint called")
    return {"data": "Bienvenue sur l'API TW3 Chat üéâ"}

@app.post("/ask", response_model=AskOut, tags=["Chat"])
async def ask_handler(payload: AskIn) -> AskOut:
    """
    ‚Ä¢ Recherche Web asynchrone (SerpAPI)  
    ‚Ä¢ Injection du contexte dans le prompt  
    ‚Ä¢ G√©n√©ration Qwen dans un thread s√©par√©
    Args:
        payload (AskIn): Contient la question et l'ID de conversation.
    Returns:
        AskOut: Contient l'ID de conversation et la r√©ponse g√©n√©r√©e.
    Raises:
        Exception: Si la g√©n√©ration √©choue ou si le contexte Web est indisponible.
    """
    conv_id = payload.conv_id or str(uuid4())
    header_dt = datetime.utcnow().strftime("%Y-%m-%dT%H%M%S")

    question = payload.question.strip()
    logger.info("Conv %s ‚Äì question : %s‚Ä¶", conv_id, question[:60])

    # 1) log QUESTION
    _append_log(conv_id, header_dt, "user", question)

    # 2) contexte Web (ne bloque pas l'event-loop CPU)
    web_ctx = await search_web(question)
    if web_ctx:
        logger.info("Web context found (%d chars)", len(web_ctx))
        prompt = (
            f"Question de l'utilisateur :\n{question}\n\n"
            f"Contexte Web (pistes externes, peut √™tre incomplet) :\n"
            f"{web_ctx}\n\n"
            "R√©ponds en t'appuyant sur ce contexte quand il est pertinent, "
            "et paraphrase les informations. Si le contexte est hors-sujet, "
            "ignore-le simplement."
        )
    else:
        prompt = question

    # 3) g√©n√©ration ‚Äì on d√©cale dans un pool thread pour ne pas bloquer l'IO
    loop = asyncio.get_running_loop()
    # answer = await loop.run_in_executor(
    #     None,
    #     lambda: generate_answer(prompt)
    # )
    answer = prompt  # Simule la g√©n√©ration pour l'exemple

    # 4) log R√âPONSE
    _append_log(conv_id, header_dt, "bot", answer)

    return AskOut(conv_id=conv_id, answer=answer)