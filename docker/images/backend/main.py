from __future__ import annotations
"""TW3 Chat Backend ‚Äì FastAPI + Qwen 7B"""

import logging
from typing import Dict
from contextlib import asynccontextmanager
from functools import lru_cache
from pathlib import Path
from uuid import uuid4
from datetime import datetime, timedelta, timezone

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import pipeline, logging as hf_logging  # type: ignore

import os
from dotenv import load_dotenv
import requests
import asyncio

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Logger ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("tw3.chat")

# R√©duit la verbosit√© des warnings Transformers
hf_logging.set_verbosity_error()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Pydantic DTO ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AskIn(BaseModel):
    question: str = Field(..., min_length=3)
    conv_id: str | None = None  # ID de conversation, None pour le 1er tour

class AskOut(BaseModel):
    conv_id: str                      # renvoy√© au front
    answer: str

# --------------------------------------------------------
load_dotenv()
API_KEY = os.getenv("NEWSAPI_KEY")
if not API_KEY:
    raise ValueError("NEWSAPI_KEY manquante.")

def format_news_context(query="Generative AI", from_date="2025-07-01", sort="relevancy", max_results=5):
    url = (f'https://newsapi.org/v2/everything?'
           f'q={query}&'
           f'from={from_date}&'
           f'sortBy={sort}&'
           f'pageSize={max_results}&'
           f'language=fr&'
           f'apiKey={API_KEY}')
    resp = requests.get(url)
    data = resp.json()
    logging.info(f"NewsAPI response: {data}")
    if data.get("status") != "ok" or "articles" not in data:
        return ""
    # On extrait un r√©sum√© format√© pour chaque article
    return "\n".join(
        f"- {art['title']} ({art.get('source', {}).get('name','')}, {art['publishedAt'][:10]}) ‚Äî {art.get('description','')}\n  {art['url']}"
        for art in data["articles"][:max_results]
    )

# dossier o√π l‚Äôon √©crit les journaux (cr√©√© au boot)
LOG_DIR = Path("/app/volume/conversations")
LOG_DIR.mkdir(parents=True, exist_ok=True)

def _append_log(conv_id: str, header_dt: str, role: str, text: str) -> None:
    """
    Ajoute une ligne au fichier de conversation.
    Cr√©e le fichier + un en-t√™te dat√© si c‚Äôest le premier appel.
    """
    file = LOG_DIR / f"conv-{conv_id}_{header_dt}.txt"
    is_new = not file.exists()
    now = datetime.now(timezone.utc)
    ts = now.isoformat(timespec="seconds").replace("+00:00", "Z")
    with file.open("a", encoding="utf-8") as f:
        if is_new:
            f.write(f"# Conversation {conv_id} ‚Äì d√©marr√©e le {header_dt}\n\n")
        f.write(f"[{ts}] {role.upper()}: {text}\n")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hugging Face pipeline (lazy‚Äëload + cache) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@lru_cache(maxsize=1)
def get_pipe():
    """Charge le mod√®le Qwen 7B une seule fois (GPU si dispo).
    Utilise un cache LRU pour √©viter de recharger le mod√®le √† chaque appel.
    Returns:
        pipeline: Instance de pipeline pour la g√©n√©ration de texte.
    """
    logger.info("Loading Qwen pipeline‚Ä¶ (first call only)")
    return pipeline(
        "text-generation",
        model="Qwen/Qwen2.5-Coder-7B-Instruct",
        device_map="auto",       # GPU si pr√©sent, sinon CPU
        trust_remote_code=True    # n√©cessaire pour Qwen
    )

def generate_answer(prompt: str,
                    max_new_tokens: int = 4096,
                    temperature: float = 0.7) -> str:
    """Appelle le mod√®le et r√©cup√®re la r√©ponse texte.
    Args:
        prompt (str): La question ou le prompt √† envoyer au mod√®le.
        max_new_tokens (int): Nombre maximum de tokens √† g√©n√©rer.
        temperature (float): Contr√¥le la cr√©ativit√© de la g√©n√©ration.
    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le.
    Raises:
        Exception: Si le mod√®le ne peut pas √™tre appel√© ou si la r√©ponse est invalide.
    """
    pipe = get_pipe()
    messages = [{"role": "user", "content": prompt}]
    out = pipe(
        messages,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
    )
    data = out[0]["generated_text"]

    # Format 1 : Qwen renvoie directement une str
    if isinstance(data, str):
        return data.strip()

    # Format 2 : liste de messages
    if isinstance(data, list):
        for msg in reversed(data):
            if isinstance(msg, dict) and msg.get("role") == "assistant":
                return str(msg.get("content", "")).strip()
        return " ".join(
            str(m.get("content", "")) if isinstance(m, dict) else str(m)
            for m in data
        ).strip()

    logger.warning("Unexpected generated_text type: %s", type(data))
    return str(data).strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Lifespan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Pr√©‚Äëcharge le pipeline au d√©marrage pour √©viter la latence du 1·µâ ≥ appel.
    Utilise un cache LRU pour garder une seule instance en m√©moire.
    Yields:
        None: Le contexte de l'application FastAPI.
    """
    logger.info("Preloading Qwen pipeline for faster responses‚Ä¶")
    get_pipe()  # d√©clenche le cache LRU
    yield

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI app ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app = FastAPI(title="TW3 Chat Backend", lifespan=lifespan)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Middleware ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@app.get("/", tags=["Health"])
def root() -> Dict[str, str]:
    """Simple endpoint de sant√©.
    Returns:
        Dict[str, str]: Message de bienvenue.
    """
    logger.info("Health check endpoint called")
    return {"data": "Bienvenue sur l'API TW3 Chat üéâ"}

@app.post("/ask", response_model=AskOut, tags=["Chat"])
async def ask_handler(payload: AskIn) -> AskOut:
    conv_id = payload.conv_id or str(uuid4())
    now = datetime.now(timezone.utc)
    header_dt = now.strftime("%Y-%m-%dT%H%M%S")
    question = payload.question.strip()
    logger.info("Conv %s ‚Äì question : %s‚Ä¶", conv_id, question)
    _append_log(conv_id, header_dt, "user", question)

    # Recherche d‚Äôactualit√©s
    from_date = (now - timedelta(days=30)).strftime("%Y-%m-%d")
    sort = "relevancy"
    max_results = 5
    news_ctx = format_news_context(
        query=question,
        from_date=from_date,
        sort=sort,
        max_results=max_results
    )
    logger.info("Conv %s ‚Äì found %d news articles", conv_id, news_ctx.count("- ") if news_ctx else 0)
    _append_log(conv_id, header_dt, "news", news_ctx or "Aucune information d‚Äôactualit√© trouv√©e.")

    if news_ctx:
        prompt = (
            "R√©ponds √† la question suivante uniquement en faisant un r√©sum√© des informations fournies et compl√®tes la r√©ponse avec tes connaissances internes si n√©cessaire."
            "Pr√©cise toujours les sources des informations utilis√©es. Tu dois restituer la source de chaque information que tu utilises dans ta r√©ponse avec sa date de publication.\n\n"
            "Ne commence jamais par une excuse de type ‚Äòen tant qu‚ÄôIA, je n‚Äôai pas acc√®s au web‚Äô.\n\n"
            f"Question : {question}\n\n"
            "Articles d‚Äôactualit√© √† exploiter :\n"
            f"{news_ctx}\n"
            "R√©ponse :"
        )
    else:
        prompt = (
            "R√©ponds √† la question suivante en t‚Äôappuyant sur tes connaissances propres. N‚Äôinvente jamais de sources, de liens, de dates ou de citations.\n"
            "Commence par identifier le th√®me principal de la question (par exemple : ‚Äúles derniers d√©veloppements en IA g√©n√©rative‚Äù) et donne une r√©ponse bas√©e uniquement sur tes connaissances internes.\n"
            "Pr√©cise alors que tu t‚Äôappuies sur tes connaissances g√©n√©rales, sans inventer ni supposer d‚Äôactualit√© pr√©cise ou de sources.\n"
            "Dans ce cas, indique aussi √† l‚Äôutilisateur que pour obtenir des informations r√©centes, il est pr√©f√©rable de poser sa question sous la forme de mots-cl√©s simples correspondant √† un th√®me, comme ‚ÄúIA g√©n√©rative‚Äù, ‚Äúcin√©ma‚Äù, ‚Äútechnologie‚Äù, etc., plut√¥t que sous forme de question complexe ou d‚Äôexemple.\n\n"
            f"Question : {question}\n\n"
            "R√©ponse :"
        ).replace("{question}", question)

    logger.info("Conv %s ‚Äì prompt : %s", conv_id, prompt)
    _append_log(conv_id, header_dt, "prompt", prompt)

    # G√©n√©ration Qwen dans un thread s√©par√©
    loop = asyncio.get_running_loop()
    answer = await loop.run_in_executor(
        None,
        lambda: generate_answer(prompt)
    )

    _append_log(conv_id, header_dt, "bot", answer)
    return AskOut(conv_id=conv_id, answer=answer)