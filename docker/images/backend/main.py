from __future__ import annotations
"""
TW3 Chat Backend ‚Äì FastAPI + Qwen 7B avec architecture modulaire

Ce module impl√©mente l'API principale du chatbot TW3 avec :
- Int√©gration du mod√®le Qwen 7B pour la g√©n√©ration de r√©ponses
- Connexion √† NewsAPI pour les actualit√©s en temps r√©el
- Cache intelligent multi-niveaux
- Patterns de resilience (circuit breaker, retry)
- Monitoring et health checks avanc√©s
- Configuration centralis√©e

Architecture:
- FastAPI pour l'API REST
- Transformers pour le mod√®le Qwen
- Modules src/ pour la logique m√©tier
- Docker pour la containerisation

Author: √âquipe TW3
Version: 1.0.0
Date: 16 juillet 2025
"""

import logging
import sys
import os
from typing import Dict, Any
from contextlib import asynccontextmanager
from functools import lru_cache
from pathlib import Path
from uuid import uuid4
from datetime import datetime, timedelta, timezone

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import pipeline, logging as hf_logging  # type: ignore

from dotenv import load_dotenv
import requests
import asyncio

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Imports des modules locaux ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Ajout du chemin src pour acc√©der aux modules de l'architecture
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../src'))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Configuration du logging ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("tw3.chat")

try:
    from config import config
    from resilience import news_api_circuit_breaker, retry_with_backoff
    from cache import cache_manager
    from monitoring import HealthCheckManager
    logger.info("Modules d'architecture charg√©s avec succ√®s")
except ImportError as e:
    logger.warning(f"Modules d'architecture non disponibles: {e}")
    # Fallback gracieux si les modules ne sont pas disponibles
    config = None
    news_api_circuit_breaker = None
    cache_manager = None

# R√©duit la verbosit√© des warnings Transformers pour un log plus propre
hf_logging.set_verbosity_error()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Mod√®les Pydantic pour l'API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class AskIn(BaseModel):
    """
    Mod√®le d'entr√©e pour l'endpoint /ask
    
    Attributes:
        question: La question de l'utilisateur (min 3 caract√®res)
        conv_id: ID de conversation optionnel pour la continuit√©
    """
    question: str = Field(..., min_length=3, description="Question de l'utilisateur")
    conv_id: str | None = Field(None, description="ID de conversation (auto-g√©n√©r√© si absent)")

class AskOut(BaseModel):
    """
    Mod√®le de sortie pour l'endpoint /ask
    
    Attributes:
        conv_id: ID de conversation (pour la continuit√©)
        answer: R√©ponse g√©n√©r√©e par le syst√®me
    """
    conv_id: str = Field(..., description="ID de conversation retourn√©")
    answer: str = Field(..., description="R√©ponse g√©n√©r√©e par le syst√®me")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Configuration et initialisation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Chargement des variables d'environnement
load_dotenv()
API_KEY = os.getenv("NEWSAPI_KEY")
if not API_KEY:
    raise ValueError("NEWSAPI_KEY environment variable is required")

# Cr√©ation du dossier de logs de conversation
LOG_DIR = Path("/app/volume/conversations")
LOG_DIR.mkdir(parents=True, exist_ok=True)
logger.info(f"R√©pertoire de logs cr√©√©: {LOG_DIR}")

# Initialisation du gestionnaire de health checks
health_manager = None
if config:
    try:
        health_manager = HealthCheckManager(API_KEY, lambda: get_pipe())
        logger.info("Health check manager initialis√©")
    except Exception as e:
        logger.warning(f"Impossible d'initialiser le health manager: {e}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Fonctions utilitaires ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _append_log(conv_id: str, header_dt: str, role: str, text: str) -> None:
    """
    Ajoute une entr√©e dans le fichier de log de conversation.
    
    Args:
        conv_id: Identifiant unique de la conversation
        header_dt: Timestamp format√© pour le nom de fichier
        role: R√¥le de l'√©metteur (user, bot, news, prompt)
        text: Contenu du message √† logger
    
    Note:
        Cr√©e automatiquement le fichier avec en-t√™te si premi√®re utilisation
    """
    file = LOG_DIR / f"conv-{conv_id}_{header_dt}.txt"
    is_new = not file.exists()
    now = datetime.now(timezone.utc)
    ts = now.isoformat(timespec="seconds").replace("+00:00", "Z")
    
    with file.open("a", encoding="utf-8") as f:
        if is_new:
            f.write(f"# Conversation {conv_id} ‚Äì d√©marr√©e le {header_dt}\n\n")
        f.write(f"[{ts}] {role.upper()}: {text}\n")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Int√©gration NewsAPI avec resilience ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# D√©corateur de retry pour les appels r√©seaux
if retry_with_backoff:
    @retry_with_backoff(max_attempts=3, exceptions=(requests.exceptions.RequestException,))
    def format_news_context(query="Generative AI", from_date="2025-07-01", sort="relevancy", max_results=5):
        """
        R√©cup√®re les actualit√©s avec cache, circuit breaker et retry automatique.
        
        Cette fonction impl√©mente plusieurs patterns de resilience :
        - Cache intelligent pour √©viter les appels r√©p√©titifs
        - Circuit breaker pour prot√©ger contre les pannes d'API
        - Retry avec backoff exponentiel pour les erreurs temporaires
        
        Args:
            query (str): Terme de recherche pour les actualit√©s
            from_date (str): Date de d√©but au format YYYY-MM-DD
            sort (str): Mode de tri ('relevancy', 'popularity', 'publishedAt')
            max_results (int): Nombre maximum d'articles √† r√©cup√©rer
            
        Returns:
            str: Articles format√©s en texte ou message d'erreur
            
        Raises:
            Exception: En cas d'erreur non r√©cup√©rable apr√®s tous les retries
        """
        # √âtape 1: V√©rification du cache intelligent
        if cache_manager and cache_manager.news_cache:
            cached_result = cache_manager.news_cache.get_news(query, from_date, sort, max_results)
            if cached_result is not None:
                logger.info(f"Cache hit pour la requ√™te NewsAPI: {query[:50]}...")
                return cached_result

        # √âtape 2: Application du circuit breaker pour protection
        if news_api_circuit_breaker:
            @news_api_circuit_breaker
            def _fetch_news():
                return _fetch_news_api(query, from_date, sort, max_results)
            
            try:
                result = _fetch_news()
                logger.info(f"NewsAPI appel√© avec succ√®s pour: {query[:50]}...")
            except Exception as e:
                logger.error(f"Circuit breaker ouvert ou erreur NewsAPI: {e}")
                return "[ERREUR] Service d'actualit√©s temporairement indisponible. Merci de r√©essayer plus tard."
        else:
            # Fallback sans circuit breaker
            result = _fetch_news_api(query, from_date, sort, max_results)
        
        # √âtape 3: Mise en cache du r√©sultat pour √©viter futurs appels
        if cache_manager and cache_manager.news_cache and not result.startswith("["):
            cache_manager.news_cache.set_news(query, from_date, sort, max_results, result)
            logger.debug(f"R√©sultat mis en cache pour: {query[:50]}...")
        
        return result
else:
    # Version simplifi√©e sans retry si module non disponible
    def format_news_context(query="Generative AI", from_date="2025-07-01", sort="relevancy", max_results=5):
        """Version simplifi√©e sans patterns de resilience."""
        if cache_manager and cache_manager.news_cache:
            cached_result = cache_manager.news_cache.get_news(query, from_date, sort, max_results)
            if cached_result is not None:
                return cached_result
        
        result = _fetch_news_api(query, from_date, sort, max_results)
        
        if cache_manager and cache_manager.news_cache and not result.startswith("["):
            cache_manager.news_cache.set_news(query, from_date, sort, max_results, result)
        
        return result

def _fetch_news_api(query: str, from_date: str, sort: str, max_results: int) -> str:
    """
    Fonction interne pour r√©cup√©rer les actualit√©s depuis NewsAPI.
    
    Cette fonction effectue l'appel HTTP r√©el vers l'API NewsAPI et traite
    la r√©ponse pour la formater en texte utilisable par le mod√®le IA.
    
    Args:
        query (str): Terme de recherche pour les actualit√©s
        from_date (str): Date de d√©but au format YYYY-MM-DD
        sort (str): Mode de tri ('relevancy', 'popularity', 'publishedAt')
        max_results (int): Nombre maximum d'articles √† r√©cup√©rer
        
    Returns:
        str: Articles format√©s en texte ou message d'erreur
        
    Raises:
        requests.exceptions.RequestException: En cas d'erreur r√©seau
        ValueError: En cas de r√©ponse malform√©e
    """
    url = (f'https://newsapi.org/v2/everything?'
           f'q={query}&'
           f'from={from_date}&'
           f'sortBy={sort}&'
           f'pageSize={max_results}&'
           f'language=fr&'
           f'apiKey={API_KEY}')
    
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"NewsAPI connection error: {e}")
        raise Exception("Impossible de se connecter √† NewsAPI. Merci de r√©essayer plus tard.")
    except Exception as e:
        logging.error(f"NewsAPI unknown error: {e}")
        raise Exception("Erreur lors du traitement de la r√©ponse NewsAPI.")

    if data.get("status") != "ok" or "articles" not in data:
        if data.get("code") == "rateLimited":
            logging.warning("NewsAPI rate limit reached")
            return "[RATE LIMIT] Le nombre maximal de requ√™tes NewsAPI a √©t√© atteint pour cette p√©riode. Merci de r√©essayer plus tard."
        logging.warning(f"NewsAPI error: {data.get('message')}")
        return "[ERREUR] NewsAPI n'a pas pu fournir d'articles pour le moment."

    if not data["articles"]:
        return ""
    
    # On extrait un r√©sum√© format√© pour chaque article
    return "\n".join(
        f"- {art['title']} ({art.get('source', {}).get('name','')}, {art['publishedAt'][:10]}) ‚Äî {art.get('description','')}\n  {art['url']}"
        for art in data["articles"][:max_results]
    )

# dossier o√π l‚Äôon √©crit les journaux (cr√©√© au boot)
LOG_DIR = Path("/app/volume/conversations")
LOG_DIR.mkdir(parents=True, exist_ok=True)

def _append_log(conv_id: str, header_dt: str, role: str, text: str) -> None:
    """
    Ajoute une ligne au fichier de conversation.
    Cr√©e le fichier + un en-t√™te dat√© si c‚Äôest le premier appel.
    """
    file = LOG_DIR / f"conv-{conv_id}_{header_dt}.txt"
    is_new = not file.exists()
    now = datetime.now(timezone.utc)
    ts = now.isoformat(timespec="seconds").replace("+00:00", "Z")
    with file.open("a", encoding="utf-8") as f:
        if is_new:
            f.write(f"# Conversation {conv_id} ‚Äì d√©marr√©e le {header_dt}\n\n")
        f.write(f"[{ts}] {role.upper()}: {text}\n")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hugging Face pipeline (lazy‚Äëload + cache) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@lru_cache(maxsize=1)
def get_pipe():
    """Charge le mod√®le Qwen 7B une seule fois (GPU si dispo).
    Utilise un cache LRU pour √©viter de recharger le mod√®le √† chaque appel.
    Returns:
        pipeline: Instance de pipeline pour la g√©n√©ration de texte.
    """
    logger.info("Loading Qwen pipeline‚Ä¶ (first call only)")
    return pipeline(
        "text-generation",
        model="Qwen/Qwen2.5-Coder-7B-Instruct",
        device_map="auto",       # GPU si pr√©sent, sinon CPU
        trust_remote_code=True    # n√©cessaire pour Qwen
    )

def generate_answer(prompt: str,
                    max_new_tokens: int = 4096,
                    temperature: float = 0.7) -> str:
    """
    Appelle le mod√®le IA et r√©cup√®re la r√©ponse texte avec cache intelligent.
    
    Cette fonction g√®re l'interaction avec le mod√®le Qwen 2.5-Coder-7B-Instruct,
    en g√©rant diff√©rents formats de r√©ponse et en optimisant les performances
    gr√¢ce au syst√®me de cache.
    
    Args:
        prompt (str): La question ou le prompt √† envoyer au mod√®le
        max_new_tokens (int): Nombre maximum de tokens √† g√©n√©rer (d√©faut: 4096)
        temperature (float): Contr√¥le la cr√©ativit√© de la g√©n√©ration (0.1-1.0, d√©faut: 0.7)
        
    Returns:
        str: La r√©ponse g√©n√©r√©e par le mod√®le, nettoy√©e et format√©e
        
    Raises:
        Exception: Si le mod√®le ne peut pas √™tre appel√© ou si la r√©ponse est invalide
        RuntimeError: Si le pipeline n'est pas initialis√© correctement
        
    Note:
        Les r√©ponses sont automatiquement mises en cache pour optimiser les performances
        sur des requ√™tes similaires.
    """
    # V√©rification du cache pour les r√©ponses du mod√®le
    if cache_manager and cache_manager.model_cache:
        cached_response = cache_manager.model_cache.get_response(prompt)
        if cached_response is not None:
            logger.info("Cache hit pour la g√©n√©ration de r√©ponse")
            return cached_response
    
    try:
        pipe = get_pipe()
        messages = [{"role": "user", "content": prompt}]
        out = pipe(
            messages,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
        )
        data = out[0]["generated_text"]

        # Format 1 : Qwen renvoie directement une str
        if isinstance(data, str):
            response = data.strip()
        # Format 2 : liste de messages
        elif isinstance(data, list):
            for msg in reversed(data):
                if isinstance(msg, dict) and msg.get("role") == "assistant":
                    response = str(msg.get("content", "")).strip()
                    break
            else:
                response = " ".join(
                    str(m.get("content", "")) if isinstance(m, dict) else str(m)
                    for m in data
                ).strip()
        else:
            logger.warning("Unexpected generated_text type: %s", type(data))
            response = str(data).strip()
        
        # Mise en cache de la r√©ponse
        if cache_manager and cache_manager.model_cache and response:
            cache_manager.model_cache.set_response(prompt, response)
        
        return response
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration: {e}")
        raise Exception(f"Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Lifespan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Pr√©‚Äëcharge le pipeline au d√©marrage pour √©viter la latence du 1·µâ ≥ appel.
    Initialise aussi les services de cache et monitoring.
    Yields:
        None: Le contexte de l'application FastAPI.
    """
    logger.info("Initialisation de l'application TW3...")
    
    # Pr√©-chargement du mod√®le
    logger.info("Preloading Qwen pipeline for faster responses‚Ä¶")
    get_pipe()  # d√©clenche le cache LRU
    
    # D√©marrage des services de cache
    if cache_manager:
        await cache_manager.start_cleanup_task()
        logger.info("Cache manager started")
    
    # D√©marrage du monitoring
    if health_manager:
        await health_manager.start_background_checks()
        logger.info("Health monitoring started")
    
    logger.info("Application TW3 initialis√©e avec succ√®s")
    
    yield
    
    # Nettoyage lors de l'arr√™t
    logger.info("Arr√™t de l'application TW3...")
    
    if cache_manager:
        await cache_manager.stop_cleanup_task()
        logger.info("Cache manager stopped")
    
    if health_manager:
        await health_manager.stop_background_checks()
        logger.info("Health monitoring stopped")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI app ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app = FastAPI(title="TW3 Chat Backend", lifespan=lifespan)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Middleware ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@app.get("/", tags=["Health"])
def root() -> Dict[str, str]:
    """Simple endpoint de sant√©.
    Returns:
        Dict[str, str]: Message de bienvenue.
    """
    logger.info("Health check endpoint called")
    return {"data": "Bienvenue sur l'API TW3 Chat üéâ"}


@app.get("/health", tags=["Health"])
async def health_check() -> Dict[str, Any]:
    """Health check d√©taill√© avec m√©triques syst√®me.
    Returns:
        Dict[str, Any]: √âtat de sant√© complet du syst√®me
    """
    if health_manager:
        return await health_manager.get_full_health_report()
    else:
        # Fallback simple si health_manager n'est pas disponible
        return {
            "status": "healthy",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "version": "1.0.0",
            "services": {
                "model": "loaded",
                "basic_check": "ok"
            }
        }


@app.get("/metrics", tags=["Monitoring"])
async def get_metrics() -> Dict[str, Any]:
    """
    Endpoint pour consulter les m√©triques de performance et de sant√© du syst√®me.
    
    Fournit des informations d√©taill√©es sur l'√©tat des caches, la sant√© des services
    externes (NewsAPI, mod√®le IA) et les statistiques d'utilisation.
    
    Returns:
        Dict[str, Any]: M√©triques compl√®tes incluant :
            - timestamp: Horodatage de la collecte
            - cache_stats: Statistiques des caches (hits, misses, √©victions)
            - health_summary: √âtat de sant√© des services externes
            
    Raises:
        HTTPException: 500 en cas d'erreur lors de la collecte des m√©triques
        
    Example:
        GET /metrics
        {
            "timestamp": "2024-01-15T10:30:00Z",
            "cache_stats": {"total_hits": 150, "total_misses": 23},
            "health_summary": {"newsapi": {"status": "healthy", "response_time_ms": 245}}
        }
    """
    try:
        metrics = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "cache_stats": cache_manager.get_global_stats() if cache_manager else {},
            "health_summary": {}
        }
        
        if health_manager:
            # Ajout des m√©triques de sant√© en cache
            for service in ['newsapi', 'model']:
                cached_health = health_manager.get_cached_health(service)
                if cached_health:
                    metrics["health_summary"][service] = {
                        "status": cached_health.status.value,
                        "response_time_ms": cached_health.response_time_ms,
                        "last_check": cached_health.last_check.isoformat() if cached_health.last_check else None
                    }
        
        return metrics
        
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des m√©triques: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la r√©cup√©ration des m√©triques")

@app.post("/ask", response_model=AskOut, tags=["Chat"])
async def ask_handler(payload: AskIn) -> AskOut:
    """
    Endpoint principal pour le chat - traite les questions utilisateur avec contexte d'actualit√©s.
    
    Ce endpoint constitue le c≈ìur de l'application TW3. Il r√©cup√®re automatiquement
    des actualit√©s pertinentes en fonction de la question pos√©e, puis g√©n√®re une
    r√©ponse contextualis√©e en utilisant le mod√®le Qwen 2.5-Coder-7B-Instruct.
    
    Workflow:
    1. Extraction et validation de la question utilisateur
    2. Recherche d'actualit√©s r√©centes li√©es au sujet via NewsAPI
    3. Construction d'un prompt enrichi avec le contexte d'actualit√©s
    4. G√©n√©ration de la r√©ponse par le mod√®le IA
    5. Logging de la conversation pour tra√ßabilit√©
    
    Args:
        payload (AskIn): Donn√©es de la requ√™te contenant :
            - question: La question pos√©e par l'utilisateur
            - conv_id: ID de conversation optionnel (g√©n√©r√© si absent)
            
    Returns:
        AskOut: R√©ponse structur√©e contenant :
            - conv_id: Identifiant unique de la conversation
            - answer: R√©ponse g√©n√©r√©e par le syst√®me
            
    Raises:
        HTTPException: 422 si la question est vide ou invalide
        HTTPException: 500 en cas d'erreur lors du traitement
        
    Example:
        POST /ask
        {
            "question": "Quelles sont les derni√®res avanc√©es en IA g√©n√©rative ?",
            "conv_id": "optional-conversation-id"
        }
        
        Response:
        {
            "conv_id": "abc123-def456-789",
            "answer": "D'apr√®s les derni√®res actualit√©s, voici les principales avanc√©es..."
        }
    """
    conv_id = payload.conv_id or str(uuid4())
    now = datetime.now(timezone.utc)
    header_dt = now.strftime("%Y-%m-%dT%H%M%S")
    question = payload.question.strip()
    logger.info("Conv %s ‚Äì question : %s‚Ä¶", conv_id, question)
    _append_log(conv_id, header_dt, "user", question)

    # Recherche d‚Äôactualit√©s
    from_date = (now - timedelta(days=30)).strftime("%Y-%m-%d")
    sort = "relevancy"
    max_results = 5
    news_ctx = format_news_context(
        query=question,
        from_date=from_date,
        sort=sort,
        max_results=max_results
    )
    logger.info("Conv %s ‚Äì found %d news articles", conv_id, news_ctx.count("- ") if news_ctx and news_ctx.startswith("-") else 0)
    _append_log(conv_id, header_dt, "news", news_ctx or "Aucune information d‚Äôactualit√© trouv√©e.")

    # --- GESTION ERREUR NEWSAPI / RATE LIMIT ---
    if news_ctx.startswith("[ERREUR]") or news_ctx.startswith("[RATE LIMIT]"):
        answer = (
            f"Erreur lors de la r√©cup√©ration des actualit√©s :\n{news_ctx}\n\n"
            "Je ne peux donc r√©pondre qu'√† partir de mes connaissances internes. "
            "Merci de r√©essayer dans quelques instants si vous souhaitez une r√©ponse bas√©e sur l‚Äôactualit√©."
        )
        _append_log(conv_id, header_dt, "bot", answer)
        return AskOut(conv_id=conv_id, answer=answer)

    if news_ctx:
        prompt = (
            "R√©ponds √† la question suivante uniquement en faisant un r√©sum√© des informations fournies et compl√®tes la r√©ponse avec tes connaissances internes si n√©cessaire."
            "Pr√©cise toujours les sources des informations utilis√©es. Tu dois restituer la source de chaque information que tu utilises dans ta r√©ponse avec sa date de publication.\n\n"
            "Ne commence jamais par une excuse de type ‚Äòen tant qu‚ÄôIA, je n‚Äôai pas acc√®s au web‚Äô.\n\n"
            f"Question : {question}\n\n"
            "Articles d‚Äôactualit√© √† exploiter :\n"
            f"{news_ctx}\n"
            "R√©ponse :"
        )
    else:
        prompt = (
            "Tu vas r√©pondre √† une question en utilisant uniquement tes connaissances internes.\n\n"
            "INSTRUCTIONS :\n"
            "- R√©ponds de mani√®re factuelle et pr√©cise.\n"
            "- N'invente jamais de sources, de liens, de dates ou de citations.\n"
            "- Indique clairement que tu t'appuies sur tes connaissances g√©n√©rales, sans acc√®s √† l'actualit√©.\n"
            "- Si la question porte sur des actualit√©s r√©centes ou des d√©veloppements tr√®s r√©cents, indique que tu ne peux pas fournir de sources d'actualit√© ou d'exemples r√©cents pr√©cis.\n"
            "- Si la question porte sur des actualit√©s r√©centes, termine ta r√©ponse comme ceci‚ÄØ:\n"
            "'Pour obtenir des informations actualis√©es, veuillez poser votre question sous forme de mots-cl√©s simples comme \"IA g√©n√©rative\", \"technologie\", \"cin√©ma\".'\n\n"
            "- **IMPORTANT‚ÄØ: Termine toujours ta r√©ponse en conseillant √† l'utilisateur de reformuler sa question en fran√ßais avec des mots-cl√©s simples comme 'IA g√©n√©rative', 'technologie', 'cin√©ma' pour obtenir des informations d'actualit√© pr√©cises.**\n\n"
            f"Question : {question}\n\n"
            "R√©ponse :"
        )

    logger.info("Conv %s ‚Äì prompt : %s", conv_id, prompt)
    _append_log(conv_id, header_dt, "prompt", prompt)

    # G√©n√©ration Qwen dans un thread s√©par√© (gestion d‚Äôerreur)
    try:
        loop = asyncio.get_running_loop()
        answer = await loop.run_in_executor(
            None,
            lambda: generate_answer(prompt)
        )
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration Qwen : {e}")
        answer = (
            "D√©sol√©, une erreur technique est survenue lors de la g√©n√©ration de la r√©ponse. "
            "Merci de r√©essayer dans quelques instants."
        )

    _append_log(conv_id, header_dt, "bot", answer)
    return AskOut(conv_id=conv_id, answer=answer)